{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 00\n",
    "## PyTorch Introduction - Part I\n",
    "\n",
    "Welcome to the introduction of PyTorch. PyTorch is a scientific computing package targeted for two main purposes: \n",
    "\n",
    "1. A replacement for NumPy with the ability to use the power of GPUs.\n",
    "2. A deep learning framework that enables the flexible and swift building of neural network models.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "### Goals of this tutorial\n",
    "\n",
    "- Getting to know PyTorch and understanding how it is different from numpy\n",
    "- Understanding PyTorch's Tensor and Pytorch's Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking PyTorch Installation and Version\n",
    "Before we begin with code, we first have to check if the correct pytorch version is installed. For this, just run the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# set printing options for nice output in this notebook\n",
    "torch.set_printoptions(profile=\"short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are almost ready to dive right into PyTorch! But first, we need to import the rest of packages required for the notebook. Such a cell is included at the top of every exercise notebook.\n",
    "\n",
    "We added the following options to the notebook to make your experience smoother:\n",
    "- %load_ext autoreload\n",
    "- %autoreload 2\n",
    "- %matplotlib inline\n",
    "\n",
    "The first two options enable a continuous reloading of the code outside of this notebook. This becomes important later on in the exercises. The last option allows for matplotlib plots to be part of the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "\n",
    "[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) is the central class of PyTorch.\n",
    "Tensors are similar to NumPyâ€™s ndarrays. The advantage of using Tensors is that \n",
    "\n",
    "* one can easily transfer them from CPU to GPU and therefore computations on tensors can be accelerated with a GPU.\n",
    "* they store additionally the gradients, if requires_grad=True is set, which is needed for efficient backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initializing Tensor\n",
    "Let us construct a NumPy array and a tensor of shape (2,3) directly from data values.\n",
    "\n",
    "The interfaces are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Numpy Array\n",
    "array_np = np.array([[1,2,3],[5,6,7]]) #NumPy array\n",
    "# Initializing the Tensor\n",
    "array_ts = torch.tensor([[1,2,3],[4,5,6]]) # Tensor\n",
    "\n",
    "print(f\"Variable array_np:\\nDatatype: {type(array_np)}\\nShape: {array_np.shape}\")\n",
    "print(f\"Values:\\n{array_np}\")\n",
    "print(f\"\\n\\nVariable array_ts:\\nDatatype {type(array_ts)}\\nShape: {array_ts.shape}\")\n",
    "print(f\"Values:\\n{array_ts.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Conversion between NumPy array and Tensor\n",
    "\n",
    "The conversion between NumPy ndarray and PyTorch tensor is quite easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion\n",
    "array_np = np.array([1, 2, 3])\n",
    "# Conversion from  a numpy array to a Tensor\n",
    "array_ts_2 = torch.from_numpy(array_np)\n",
    "\n",
    "# Conversion from  Tensor to numpy array\n",
    "array_np_2 = array_ts_2.numpy() \n",
    "\n",
    "# Change a value of the np_array\n",
    "array_np_2[1] = -1 \n",
    "\n",
    "# Changes in the numpy array will also change the values in the tensor\n",
    "assert(array_np[1] == array_np_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b></b> During the conversion, both ndarrays and the Tensor share the same memory address. Changes in value of one will\n",
    "affect the other.</div>\n",
    "\n",
    "## 1.3 Operations on Tensor\n",
    "\n",
    "### 1.3.1 Indexing\n",
    "\n",
    "We can use the NumPy array-like indexing for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take the first two columns from the original tensor array and save it in a new one\n",
    "b = array_ts[:2, :2] \n",
    "\n",
    "# Let's assign the value of first column of the new variable to be zero \n",
    "b[:, 0] = 0 \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now select elements which satisfy a particular condition. In this example, let's find those elements of tensor which are array greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the elements with value greater than one\n",
    "mask = array_ts > 1 \n",
    "new_array = array_ts[mask]\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try performing the same operation in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = array_ts[array_ts>1]\n",
    "\n",
    "# Is the result same as the array from the previous cell?\n",
    "print(c == new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Mathematical operations on Tensor\n",
    "\n",
    "\n",
    "#### Element-wise operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "# Addition - Syntax 1\n",
    "print(f\"x + y: \\n{(x + y).cpu().numpy()}\")\n",
    "\n",
    "# Addition - Syntax 2\n",
    "print(f\"x + y: \\n{torch.add(x, y).cpu().numpy()}\")\n",
    "\n",
    "# Addition - Syntax 3\n",
    "result_add = torch.empty(2, 2)\n",
    "torch.add(x, y, out=result_add)\n",
    "print(f\"x + y: \\n{result_add.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We only added `.cpu().numpy()` to receive a better formatted print statement.\n",
    "\n",
    "Similar syntax holds for other element-wise operations such as subtraction and multiplication.\n",
    "\n",
    "When dividing two integers in NumPy as well PyTorch, the result is always a **float**.   \n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([[1,2],[3,4]])\n",
    "y_np = np.array([[5,6],[7,8]])\n",
    "print(f\"x / y: \\n{x_np / y_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "PyTorch offers different options for doing matrix matrix multiplication.\n",
    "\n",
    "If you want to do matrix mupliplication with more then two tensors you can use [torch.einsum()](https://pytorch.org/docs/stable/generated/torch.einsum.html). Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(3, 3)\n",
    "tensor2 = torch.randn(3)\n",
    "\n",
    "# Matrix Multiplication - Syntax 1\n",
    "output1 = tensor1 @ tensor2\n",
    "# Matrix Multiplication - Syntax 2\n",
    "output2 = torch.matmul(tensor1, tensor2)\n",
    "# Matrix Multiplication - Syntax 3\n",
    "output3 = torch.einsum(\"ij,j->i\", tensor1, tensor2)\n",
    "\n",
    "print(f\"Matrix mutlplication\\nInputs:\\n{tensor1.cpu().numpy()}\\nand\\n{tensor2.cpu().numpy()} \\n\\n\",\n",
    "      f\"Output1: \\n{output1.cpu().numpy()}\\n\",\n",
    "      f\"Output2: \\n{output2.cpu().numpy()}\\n\",\n",
    "      f\"Output3: \\n{output3.cpu().numpy()}\")\n",
    "\n",
    "assert output1.equal(output2)\n",
    "assert output2.equal(output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing matrix multiplication with more than two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3, 3)\n",
    "tensor3 = torch.randn(3)\n",
    "# Matrix Multiplication - Syntax 1\n",
    "output1 = tensor1 @ tensor2 @ tensor3\n",
    "# Matrix Multiplication - Syntax 2\n",
    "output2 = torch.einsum(\"i,ij,j\", tensor1, tensor2, tensor3)\n",
    "\n",
    "print(f\"Chain multiplication:\\n{output1}\\n{output2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Broadcasting\n",
    "\n",
    "Broadcasting is very important in PyTorch often leading to cleaner code. The idea of broadcasting is to enable cleaner code by automatically expanding tensors to the right shape. This removes the necessity of doing these operations by hand in the code itself, which is often confusing and introduces potential bugs in the code. Broadcasting is also available in Numpy, but in PyTorch it is often quite handy as operations are often done over a batch. We show some examples for broadcasting with matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "print(\n",
    "    f\"vector x vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\", \n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"matrix x vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"batched matrix x broadcasted vector multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "print(\n",
    "    f\"batched matrix x batched matrix multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")\n",
    "\n",
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "print(\n",
    "    f\"batched matrix x broadcasted matrix multiplication:\\n\",\n",
    "    f\"Input shapes:\\n\",\n",
    "    f\"{[size for size in tensor1.size()]} and {[size for size in tensor2.size()]}\\n\",\n",
    "    f\"Output shape:\\n\",\n",
    "    f\"{[size for  size in torch.matmul(tensor1, tensor2).size()]}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting often goes hand in hand with the `Tensor.unsqueeze(dim)` method. `Tensor.unsqueeze(dim)` returns a Tensor with an additional dimension of size 1 inserted at `dim`. We show the use of this together in use with tensor addition. For addtional mathematical operations check out the [PyTorch](https://pytorch.org/docs/stable/index.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a vector to each row of a matric\n",
    "tensor1 = torch.randn(2, 3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(\n",
    "    f\"adding a vector to each row of a matrix:\\n\",\n",
    "    f\"Inputs:\\n\", \n",
    "    f\"{tensor1.cpu().numpy()}\\n\",\n",
    "    f\"{tensor2.cpu().numpy()}\\n\",\n",
    "    f\"Outputs:\\n\",\n",
    "    f\"{(tensor1 + tensor2.unsqueeze(0)).cpu().numpy()}\\n\"\n",
    ")\n",
    "\n",
    "# adding each element of a vector to each element of another vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(4)\n",
    "print(\n",
    "    f\"adding each element of a vector to each element of another vector:\\n\",\n",
    "    f\"Inputs:\\n\", \n",
    "    f\"{tensor1.cpu().numpy()}\\n\",\n",
    "    f\"{tensor2.cpu().numpy()}\\n\",\n",
    "    f\"Outputs:\\n\",\n",
    "    f\"{(tensor1.unsqueeze(1) + tensor2.unsqueeze(0)).cpu().numpy()}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Gradients\n",
    "\n",
    "We create two tensors a and b with requires_grad=True. This signals to `autograd` that every operation on them should be tracked. We create another tensor ``Q`` from ``a`` and ``b``. \n",
    "\n",
    "$Q = 3a^3 - b^2$\n",
    "\n",
    "`autograd` then let us compute the gradient of ``Q`` with respect to ``a`` and ``b``. In this case\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = -2b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "# compute the a function with the pytorch tensors\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "# call backward on a function to compute the gradient\n",
    "Q.sum().backward()\n",
    "print(f\"Gradients:\\na:\\n{a.grad.cpu().numpy()}\\nb:\\n{b.grad.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable the gradient computation for single tensors by setting `requires_grad=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=False)\n",
    "\n",
    "# compute the a function with the pytorch tensors\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "# call backward on a function to compute the gradient\n",
    "Q.sum().backward()\n",
    "print(f\"Gradients:\\na:\\n{a.grad.cpu().numpy()}\\nb:\\n{b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing evaluations you can wrap a code block in \n",
    "`with torch.no_grad()`\n",
    "to prevent gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  a = torch.tensor([2., 3.], requires_grad=True)\n",
    "  b = torch.tensor([6., 4.], requires_grad=False)\n",
    "\n",
    "  # compute the a function with the pytorch tensors\n",
    "  Q = 3*a**3 - b**2\n",
    "\n",
    "  # call backward with torch.no_grad() enabled results in a runtime error\n",
    "  try:\n",
    "    Q.sum().backward()\n",
    "  except RuntimeError as e:\n",
    "    print(f\"RuntimeError: {e}\")\n",
    "  print(f\"Gradients:\\na:\\n{a.grad}\\nb:\\n{b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Devices\n",
    "\n",
    "When training a neural network, it is important to make sure that all the required tensors as well as the model are on the same device. Tensors can be moved between the CPU and GPU using `.to` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if a GPU is available. If it is available, we will assign it to `device` and move the tensor `x` to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The selected device is {device}\")\n",
    "\n",
    "x = torch.rand(3, 4, device=\"cpu\")\n",
    "print(f\"Original device: {x.device}\") # \"cpu\"\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {tensor.device}\") #\"cpu\" or \"cuda\"\n",
    "\n",
    "print(f\"The original Tensor remains on: {x.device}\") # \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `.to(device)` we have created a copy of the tensor on our selected device. This is a CUDA device for those who have a GPU; otherwise it's still on the CPU. `x` however, remains on the CPU.\n",
    "\n",
    "You might have noticed that the device for the tensor is `cuda:0` instead of `cuda`. The `:0` signals that the tensor is on the first GPU. This becomes very important when you start to do distributed training on multiple GPUs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Try including the <b>.to(device)</b> calls in your codes. It is then easier to port the code to run on a GPU.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Timing with PyTorch\n",
    "\n",
    "Timing CPU-only operations can be done with standard python timing operations, e.g. timeit.\n",
    "\n",
    "Since CUDA is asynchronous, timing GPU operations needs some additional tools. One option uses CUDA events. Timing the matrix multiplication is done by sandwiching the call between CUDA events.\n",
    "\n",
    "Other timing options that use the PyTorch [autograd profiler](https://pytorch.org/docs/stable/autograd.html?highlight=autograd%20profiler#torch.autograd.profiler.profile) are also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# create random variables to do matrix multiplication with\n",
    "A = torch.randn((10, 10000, 10000), device=\"cpu\")\n",
    "b = torch.randn((10000, 1), device=\"cpu\")\n",
    "\n",
    "start_cpu = time.perf_counter()\n",
    "results_cpu = A @ b\n",
    "end_cpu = time.perf_counter()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "print(f\"Time with cpu in sec: \\n{end_cpu - start_cpu}\")\n",
    "\n",
    "A = A.to(device)\n",
    "b = b.to(device)\n",
    "\n",
    "# create a start and end cuda event used for timing\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "results_gpu = A @ b\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Timing with {device} in sec: \\n{start.elapsed_time(end) / 1000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cv3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b38045e10271186d31b9c7cfcf32f44b81f9b46f72bad763493647421023d2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
