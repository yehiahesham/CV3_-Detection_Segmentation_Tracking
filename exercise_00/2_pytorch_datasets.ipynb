{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7x57SEGcohj"
   },
   "source": [
    "# Exercise 00\n",
    "## PyTorch Introduction - Part II\n",
    "\n",
    "### Goals of this tutorial\n",
    "\n",
    "- Understanding PyTorch's approach to loading data from storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJASSlp3Q68l"
   },
   "source": [
    "## 2.1 The Datasets class\n",
    "\n",
    "In pytorch, the Dataset class primitive is the main way to access the data you are working with. Individual and customized Dataset classes are derived from the base class. The full functionality of the Dataset class is captured in the three dunder methods:\n",
    "- `__init__` is run once and instantiates the dataset by preloading all relevant information for the dataset\n",
    "- `__len__` defines how many datapoints/samples are in the dataset\n",
    "- `__getitem__` defines how to load an individual datasample and defines the structure of the output\n",
    "\n",
    "Each derived Dataset class must implement these methods. \n",
    "\n",
    "### 2.1.1 Creating a custom Dataset\n",
    "For these examples, we are working with small datasets. Therefore, we can preload them completely and store them in our memory. This usually does not work, especially when working with image data. You will see a proper example in the next notebook. However, for demonstration purposes, we only work with a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1666247844878,
     "user": {
      "displayName": "Dominik Muhle",
      "userId": "05194695185902771730"
     },
     "user_tz": -120
    },
    "id": "XvY1ucAmCxGC"
   },
   "outputs": [],
   "source": [
    "a = 1.0\n",
    "b = 2.0\n",
    "c = 3.0\n",
    "\n",
    "# X represents the input data, y the target values\n",
    "X = torch.linspace(-10, 10, 1000)\n",
    "y = a * X**2 + b * X + c + 2.0 * torch.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaVsPRv2Fqk5"
   },
   "source": [
    "The following class represents our dataset. In this case we initialize the object by passing the input data and the target values directly to the ``__init__`` function. However, this depends on the dataset that is used. Especially with larger datasets it is infeasible to store it as a whole in the object. The [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) provides an example for more advances custom dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1666247915961,
     "user": {
      "displayName": "Dominik Muhle",
      "userId": "05194695185902771730"
     },
     "user_tz": -120
    },
    "id": "2v0GoS9tDM3G"
   },
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "  def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "    # store the input data\n",
    "    self.X = X\n",
    "    # store the target values\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    # get the number of datapoints N from the target value tensor with shape N\n",
    "    return self.y.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # returns the datapoint at the index as a tuple of input data, output value\n",
    "    return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfzVc6TrDld0"
   },
   "source": [
    "### 2.1.2 Visualizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1666247927900,
     "user": {
      "displayName": "Dominik Muhle",
      "userId": "05194695185902771730"
     },
     "user_tz": -120
    },
    "id": "OzzF9bg8DtrM",
    "outputId": "909e7054-3cee-4b30-c6e5-7022af3aa103"
   },
   "outputs": [],
   "source": [
    "dataset = RegressionDataset(X, y)\n",
    "\n",
    "# visualization of the dataset\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plt.title('full dataset')\n",
    "ax.scatter(dataset.X, dataset.y, s=1.0)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWNIjaH3FHKH"
   },
   "source": [
    "### 2.1.3 Splitting the dataset into train/test split\n",
    "\n",
    "In machine learning tasks you often want to evaluate your model on unseen data. The whole dataset is therefore split into a training and a test split. PyTorch offers you different options to split your dataset into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1666248026172,
     "user": {
      "displayName": "Dominik Muhle",
      "userId": "05194695185902771730"
     },
     "user_tz": -120
    },
    "id": "IfFYG5LGFGbv",
    "outputId": "042700ee-cb3d-458c-a4cd-7470456194d9"
   },
   "outputs": [],
   "source": [
    "train_len = (int) (0.8 * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "# creates multiple PyTorch Subset that hold the different splits\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# visualizing the different splits\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "# accessing the data isn't as trivial as before\n",
    "axs[0].scatter(dataset.X[train_data.indices], dataset.y[train_data.indices], s=1.0)\n",
    "axs[0].set_title(\"trainings split\")\n",
    "axs[0].set_xlabel(\"X\")\n",
    "axs[0].set_ylabel(\"y\")\n",
    "# accessing the data isn't as trivial as before\n",
    "axs[1].scatter(dataset.X[test_data.indices], dataset.y[test_data.indices], s=1.0, color='tab:orange')\n",
    "axs[1].set_title(\"test split\")\n",
    "axs[1].set_xlabel(\"X\")\n",
    "axs[1].set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3wDnrgQJO50"
   },
   "source": [
    "### 2.1.4 Working with dataloaders\n",
    "\n",
    "PyTorch offers a convient way to obtain batches from a ``Dataset`` object by wrapping an iterable around it in the form of an ``DataLoader``. An overview over the different parameters can be found in the [documentation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). A `DataLoader` aggregates the output of the `__get_item__` method along a new batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1666248128541,
     "user": {
      "displayName": "Dominik Muhle",
      "userId": "05194695185902771730"
     },
     "user_tz": -120
    },
    "id": "JXG536tsHzUZ",
    "outputId": "85957d5b-28f9-43c4-e0e7-e8d6b0871c6b"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# create the python iterables for the datasets\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# obtain the next batch from the training split\n",
    "data = next(iter(train_dataloader))\n",
    "X_train, y_train = data\n",
    "\n",
    "# visualize the whole batch\n",
    "axs[0].scatter(X_train, y_train, s=100.0)\n",
    "# visualize the first entry of the batch\n",
    "axs[0].scatter(X_train[0], y_train[0], s=100.0, color=\"tab:orange\")\n",
    "axs[0].set_title(\"trainings split\")\n",
    "axs[0].set_xlabel(\"X\")\n",
    "axs[0].set_ylabel(\"y\")\n",
    "\n",
    "# obtain the next batch from the test split\n",
    "data = next(iter(test_dataloader))\n",
    "X_test, y_test = data\n",
    "\n",
    "# visualize the whole batch\n",
    "axs[1].scatter(X_test, y_test, s=100.0)\n",
    "# visualize the first entry of the batch\n",
    "axs[1].scatter(X_test[0], y_test[0], s=100.0, color=\"tab:orange\")\n",
    "axs[1].set_title(\"test split\")\n",
    "axs[1].set_xlabel(\"X\")\n",
    "axs[1].set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 An in-depth Look at `__get_item__`\n",
    "\n",
    "The previous example was quite simple. We had a single scalar value as an input and a single scalar value as an output. When working with images, this is often not realistic. Our trainings data often consists of either grayscale or RGB images with ground truth that can vary from semantic annotations, to instance annotations and even camera pose transformation matrices. Sometimes, it is necessary to also return some meta-information of the datasample. In these cases it is not enough to simply return a tuple of two values.\n",
    "\n",
    "In the following we will look at all the different ways, pytorch allows you to return datastructures from a dataset.\n",
    "\n",
    "### 2.1.1 Returning non-tensor data\n",
    "\n",
    "The dataloaders in pytorch allow your dataset to return data as most standard python datastructures, including `List`, `Tuple`, and `Dict`. But also allows to return `str`, `np.ndarray`. The next cell shows a `Dataset` that covers all these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "  def __init__(self, num_data=100):\n",
    "    self.torch_data = torch.randn((num_data, 2))\n",
    "    self.numpy_data = np.random.rand(num_data, 2)\n",
    "    self.int_data = [idx for idx in range(num_data)]\n",
    "    self.str_data = [f\"{idx}\" for idx in range(num_data)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.torch_data.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # in the following B will denote the batch size\n",
    "    return (\n",
    "        # for torch tensors the dataloader will return a torch.Tensor with size \n",
    "        # B,... where ... is the shape of your data\n",
    "        self.torch_data[index],\n",
    "        # for numpy arrays the dataloader will convert the arrays to a \n",
    "        # torch.Tensor with size B,... where ... is the shape of your data\n",
    "        self.numpy_data[index],\n",
    "        # for integers, floats, etc. the dataloader will return a torch.Tensor \n",
    "        # with size B\n",
    "        self.int_data[index],\n",
    "        # for strings the dataloader will return a list of length B\n",
    "        self.str_data[index],\n",
    "        # for list the dataloader will accumulate the data for each element of \n",
    "        # the list according to the previous rules\n",
    "        [self.torch_data[index], self.numpy_data[index]], \n",
    "        # for dict the dataloader will accumulate the data for each element of \n",
    "        # the dict according to the previous rules and keep the keys\n",
    "        {\"torch\": self.torch_data[index], \"numpy\": self.numpy_data[index]},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Using a dataloader\n",
    "\n",
    "Based on the output structure, the pytorch `DataLoader` collates the data from the dataset output. You can familiarise yourself with the different options in the [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#).\n",
    "\n",
    "The ``DataLoader`` automatically accumulates your training data along a batch dimension while trying to preserve the datastructure. If you need a more specific aggregation of data, you can always define a custom `collate_fn` yourself. However, this is an advanced topic we will not cover this, as it is rarely needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = DummyDataset()\n",
    "dataloader = DataLoader(dummy_dataset, batch_size=8)\n",
    "\n",
    "print(f\"Created a dataset with {len(dummy_dataset)} entries.\")\n",
    "print(f\"Created a dataloader with {len(dataloader)} batches.\")\n",
    "\n",
    "for batch_idx, data in enumerate(dataloader):\n",
    "  torch_data = data[0]\n",
    "  print(f\"torch data:\\n{torch_data}\\n\", f\"shape:\\n{torch_data.shape}\")\n",
    "  numpy_data = data[1]\n",
    "  print(f\"numpy data:\\n{numpy_data}\\n\", f\"shape:\\n{numpy_data.shape}\")\n",
    "  int_data = data[2]\n",
    "  print(f\"int data:\\n{int_data}\\n\", f\"shape:\\n{int_data.shape}\")\n",
    "  str_data = data[3]\n",
    "  print(f\"str data:\\n{str_data}\")\n",
    "  list_data = data[4]\n",
    "  print(f\"list data:\\n{list_data}\")\n",
    "  dict_data = data[5]\n",
    "  print(f\"dict data:\\n{dict_data}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h6ffzMtcoh4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. [PyTorch Tutorial](https://pytorch.org/tutorials/)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "17RqclroXTQLjK8oKvhf3_YgX4nx2lA7h",
     "timestamp": 1666019201411
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cv3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b38045e10271186d31b9c7cfcf32f44b81f9b46f72bad763493647421023d2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
